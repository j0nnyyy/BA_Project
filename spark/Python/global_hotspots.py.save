import hotspot_detection
import load_to_spark
import argparse

base_path = "/scratch/wikipedia-dump/wiki_small_"
bots = ["Bot", "Bots"]
window_size = "2"
multiplier = 2
filenames = []

parser = argparse.ArgumentParser()
parser.add_argument("--filenumber")
parser.add_argument("--filecount")
parser.add_argument("--windowsize", help="the window size in weeks")
parser.add_argument("--multiplier", help="the multiplier to identify hotspots")
args = parser.parse_args()

if args.filecount:
    if args.filecount > 1:
        for i in range(1, int(args.filecount) + 1):
            filenames.append(base_path + str(i) + ".json")
    else:
        filenames.append(base_path + "1.json")
elif args.filenumber:
    filenames.append(base_path + args.filenumber + ".json")
else:
    filenames.append(base_path + "1.json")

if args.windowsize:
    window_size = args.windowsize

if args.multiplier:
    multiplier = int(args.multiplier)

def init_df():
    df = load_to_spark.init_article_hotspot_df(filenames)
    df = df.where(col("author").isNotNull())
    df_bots = df.where(col("author").rlike("|".join(bots)))
    df = df.subtract(df_bots)
    print(df.count())
    return df




init_df()
